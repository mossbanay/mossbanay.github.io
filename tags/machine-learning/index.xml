<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning on Moss&#39; blog</title>
    <link>https://banay.me/tags/machine-learning/</link>
    <description>Recent content in Machine-Learning on Moss&#39; blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Moss Ebeling</copyright>
    <lastBuildDate>Tue, 30 Jun 2020 21:01:00 +1000</lastBuildDate>
    <atom:link href="https://banay.me/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Debugging some GANs</title>
      <link>https://banay.me/debugging-a-gan/</link>
      <pubDate>Tue, 30 Jun 2020 21:01:00 +1000</pubDate>
      <guid>https://banay.me/debugging-a-gan/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Recently I&amp;rsquo;ve been motivated to investigate generative models, of which the&#xA;most popular is currently GANs. The best way I could think of learning about&#xA;GANs in deeper detail (since I&amp;rsquo;m interested in tweaking them later in some&#xA;applied cases) is to implement them myself, and solve the bugs and issues that&#xA;arise in practice myself.&lt;/p&gt;&#xA;&lt;p&gt;In this article I&amp;rsquo;ll use PyTorch and a framework that helps simplify training&#xA;called PyTorch Lightning &lt;falcon2019pytorch&gt;. PyTorch Lightning allow&#xA;separation of training code from network architecture code. It also exposes&#xA;useful callback methods to quickly add different logging or debugging which can&#xA;be useful when training GANs, which is often a non-trivial process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Double) Q-learning and maximisation bias</title>
      <link>https://banay.me/maximisation-bias-q-learning/</link>
      <pubDate>Thu, 30 Apr 2020 16:36:00 +1000</pubDate>
      <guid>https://banay.me/maximisation-bias-q-learning/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In this article we&amp;rsquo;ll review Q-learning and walk through a subtle improvement&#xA;that leads to Double Q-learning and better policies. We&amp;rsquo;ll then look at this in&#xA;action, and compare the two methods on a toy problem.&lt;/p&gt;&#xA;&lt;h2 id=&#34;reinforcement-learning-refresher&#34;&gt;Reinforcement learning refresher&lt;/h2&gt;&#xA;&lt;p&gt;In this article I assume a familiarity with reinforcement learning and the&#xA;standard Q-learning algorithm. In this section I&amp;rsquo;ll provide a brief review of&#xA;the basic terminology, which you can feel free to skip if you&amp;rsquo;re comfortable in&#xA;doing so.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Let&#39;s write a Neural Arithmetic Logic Unit</title>
      <link>https://banay.me/nalu/</link>
      <pubDate>Tue, 14 Jan 2020 17:46:00 +1100</pubDate>
      <guid>https://banay.me/nalu/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;A few months ago I read &lt;a href=&#34;https://arxiv.org/abs/1808.00508&#34;&gt;this paper&lt;/a&gt; from DeepMind that addressed a simple choice&#xA;of architecture to encourage sensible weights in neural networks when solving&#xA;problems that at the core are simple arithmetic. Despite the continued hype&#xA;surrounding neural networks and deep learning in general, some simple problems&#xA;like this are difficult to generalise past the regions used in a training set.&lt;/p&gt;&#xA;&lt;p&gt;XOR was a famously difficult problem that stunted developments in perceptrons&#xA;(the predecessors to what has become neural networks) until Marvin Minsky and&#xA;Seymour Papert addressed it by applying composition to the model. In a similar&#xA;vein to the NALU paper, the value added is the proposition of a new architecture&#xA;since single-layer perceptrons are inherently linear, there&amp;rsquo;s no way to solve&#xA;XOR without a multiple layers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Playing Tic-tac-toe with minimax in Python</title>
      <link>https://banay.me/tic-tac-toe-minimax/</link>
      <pubDate>Sun, 19 May 2019 14:36:00 +1000</pubDate>
      <guid>https://banay.me/tic-tac-toe-minimax/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;In this article we will explain the minimax algorithm. We&amp;rsquo;ll cover game trees, the minimax algorithm itself and a simple implementation in Python. We&amp;rsquo;ll also review some popular extensions that speed up or improve upon the actions taken by minimax.&lt;/p&gt;&#xA;&lt;h2 id=&#34;game-trees&#34;&gt;Game trees&lt;/h2&gt;&#xA;&lt;p&gt;For games with perfect information, we can model the entire play-space using a directed graph called &lt;em&gt;game tree&lt;/em&gt;. A game tree simply illustrates all possible ways in which a game may play out. Each node described a particular state in the game, and it has one child node for each possible action that might occur from that state. To generate a game tree, we need only the rules of the game as inputs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hyperparameter selection with T-tests</title>
      <link>https://banay.me/hyperparameter-t-test/</link>
      <pubDate>Sun, 11 Nov 2018 19:14:00 +1100</pubDate>
      <guid>https://banay.me/hyperparameter-t-test/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;One of the most important steps in developing a model for machine learning is tuning hyperparameters to ensure it generalises to unseen data. A model that fits the training set well but performs poorly on anything else is useless, so care should be taken in ensuring that in-sample performance characteristics of a model are representative of real world performance also. The most simple of these methods is of course using a holdout set, which allows for an easy way to estimate performance on out-sample data but when used as part of the model iteration process, is usually is also overfit to some degree.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
